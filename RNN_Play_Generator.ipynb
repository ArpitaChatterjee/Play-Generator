{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN Play Generator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNTW9k5POKo53UJTOAfTnwo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArpitaChatterjee/RNNs-Play-Generator/blob/main/RNN_Play_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9cMnsfI1tFK"
      },
      "source": [
        "### **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL5zeW-k1C2Z",
        "outputId": "96b3b51c-3530-4ce4-8ebb-cde17eec3a98"
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNuLBKvy1yC_"
      },
      "source": [
        "### **Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBGLPXCf1RWm",
        "outputId": "9ecab36f-8eeb-494c-f8f3-5fd877c7d145"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "86LgxxMT_Ek1",
        "outputId": "61dd86da-c597-4add-8595-494dfa97008d"
      },
      "source": [
        "#Loading Your Own Data\n",
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0a0d1676-c038-4346-ab73-f9c5c1d55d2a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0a0d1676-c038-4346-ab73-f9c5c1d55d2a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-46676ad65841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Loading Your Own Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpath_to_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlkRKI_B126c"
      },
      "source": [
        "### **Read Contents of File**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCR1771o1V5O",
        "outputId": "4a94db24-926a-4e3b-ca62-bdede8fa6693"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsB5NWg51fZ8",
        "outputId": "00ce703a-c8f5-473f-eb0c-3e3af7b6d3ec"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYSuOFq718zj"
      },
      "source": [
        "### **Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUYGtU0q1q1t"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otnooZ1r2DSf",
        "outputId": "e235889e-cf68-4400-f970-2e80000badc3"
      },
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWVyvh3a2R5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1529e452-f0cd-4814-f45e-ba95ff32a30d"
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQb9vrZX_jcU"
      },
      "source": [
        "### **Creating Training Examples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBZ7ceuA_NJD"
      },
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-bIek4t_2eR"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqP08lkK_7h4"
      },
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BMTLZXc_8NK",
        "outputId": "80326549-0d1d-4202-b2e6-88f94437d7a7"
      },
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBYco77LACIk"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9YaPvO3ADD6"
      },
      "source": [
        "## **Building the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6YRmZLtX0d0"
      },
      "source": [
        "### Now it is time to build the model. We will use an embedding layer a LSTM and one dense layer that contains a node for each unique character in our training data. The dense layer will give us a probability distribution over all nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0woLCJj5AN5P",
        "outputId": "4dfdb3b4-c8b7-447f-8b7c-a6bf352d9c32"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, ##256\n",
        "                              batch_input_shape=[batch_size, None]),##none==wedont know how long the sequences will be in each batch\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,#return intermidiate steps , otherwise model shows last output not inbtwn steps\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'), #what the values will take in lstm at start \n",
        "    tf.keras.layers.Dense(vocab_size) #final layer-> noofnodes==noofcharacter in the text\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQcuY-1SAWrA"
      },
      "source": [
        "## **Creating a Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkJxHDMDAVnn",
        "outputId": "dbf96a74-5fda-4443-9b09-4b3915d4d6b1"
      },
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP7t_zpVBE8J",
        "outputId": "b3a52a20-fd96-4fa3-ecad-8ee43217fc1f"
      },
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-1.89703819e-03 -5.89703955e-03 -2.00884975e-03 ... -7.52128690e-05\n",
            "   -4.80467686e-03 -5.56411571e-04]\n",
            "  [-3.99279204e-04 -6.33597793e-03  2.12126481e-03 ... -5.80863329e-04\n",
            "    6.82282320e-04 -1.62663600e-05]\n",
            "  [ 1.48616137e-03 -1.05754624e-03  8.67999811e-03 ...  6.24921056e-04\n",
            "    7.11157976e-04  1.37365132e-03]\n",
            "  ...\n",
            "  [-2.03353423e-03 -8.39139987e-03  1.38957892e-02 ...  8.81619286e-03\n",
            "   -1.42411806e-03  1.87050540e-03]\n",
            "  [-1.15967426e-03 -2.24682782e-03  1.02629755e-02 ...  1.29282465e-02\n",
            "   -2.53062695e-04  2.40949541e-03]\n",
            "  [-1.05780887e-03 -7.89244287e-03  5.86100295e-03 ...  9.71492846e-03\n",
            "   -5.48229972e-03  2.95950507e-04]]\n",
            "\n",
            " [[-2.22941558e-03  1.83767857e-04 -2.68761721e-03 ...  3.92024219e-03\n",
            "   -1.45832298e-03  1.55246141e-03]\n",
            "  [-8.38356931e-03  5.81982220e-03  3.66874365e-03 ...  5.72699076e-03\n",
            "    9.30211041e-04  4.73548798e-03]\n",
            "  [-9.05497558e-03  4.71149944e-03 -7.80117116e-04 ...  8.64975061e-03\n",
            "   -5.01705334e-04  5.22391079e-03]\n",
            "  ...\n",
            "  [-9.07889334e-05 -6.58534095e-03  6.36375137e-03 ... -1.00803903e-04\n",
            "   -1.05197700e-02  1.96527876e-03]\n",
            "  [ 2.26119155e-04 -1.49677577e-03  5.44550316e-03 ...  5.52249979e-03\n",
            "   -7.87085667e-03  3.79998237e-03]\n",
            "  [-4.86069266e-03 -3.44897318e-03  7.81069277e-03 ...  4.58653085e-03\n",
            "   -7.72566115e-03  4.35688859e-03]]\n",
            "\n",
            " [[ 2.54299899e-04 -3.22097447e-03  7.87925150e-04 ...  2.51678028e-03\n",
            "    1.78987090e-03  9.11873416e-04]\n",
            "  [-1.84285280e-03 -7.55788572e-03 -2.48031248e-03 ...  2.59865006e-03\n",
            "   -3.89511976e-03 -3.68234760e-04]\n",
            "  [ 2.26963122e-04 -6.07531751e-04 -3.19445855e-04 ...  2.31190724e-03\n",
            "   -4.90753818e-03 -1.39192597e-03]\n",
            "  ...\n",
            "  [-3.96873243e-03 -1.65215544e-02  1.57597326e-02 ...  5.42101311e-03\n",
            "   -1.48935020e-02  2.03099544e-03]\n",
            "  [-3.34049296e-03 -1.67539883e-02  1.41210547e-02 ...  7.17085646e-03\n",
            "   -1.06442682e-02  1.21167407e-03]\n",
            "  [-7.79366121e-03 -9.57204681e-03  1.32373413e-02 ...  2.90315691e-03\n",
            "   -5.12368884e-03  4.61438997e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.42601435e-03  9.44671454e-04 -8.58761661e-04 ...  1.45680585e-03\n",
            "   -3.06740217e-03  3.63502512e-03]\n",
            "  [ 2.79964623e-03 -8.33826081e-04  2.35518510e-03 ...  1.35239505e-03\n",
            "    2.73729209e-03  3.53856571e-03]\n",
            "  [ 4.01069270e-03 -4.63482598e-03  1.82338676e-03 ... -1.95531757e-03\n",
            "    4.80774837e-03  4.59771603e-03]\n",
            "  ...\n",
            "  [-5.30242920e-03 -8.70009512e-03 -6.15178258e-04 ...  4.91552893e-03\n",
            "   -1.51027665e-02  1.04495157e-02]\n",
            "  [-5.43125253e-03 -1.16996570e-02 -2.58114445e-03 ...  4.07061959e-03\n",
            "   -1.73492022e-02  7.88401067e-03]\n",
            "  [-3.82028148e-03 -5.15087787e-03 -1.50146708e-03 ...  8.85702018e-03\n",
            "   -1.38017917e-02  8.94829165e-03]]\n",
            "\n",
            " [[ 9.92832822e-04 -4.25775815e-03  8.09970788e-06 ... -3.22114211e-03\n",
            "    2.01377692e-03  2.02072621e-03]\n",
            "  [-1.45499641e-03 -3.36980796e-03  1.61041634e-03 ... -2.21887464e-03\n",
            "    9.71972011e-04 -2.18655542e-03]\n",
            "  [-3.41278943e-03 -8.54421034e-03 -8.70334741e-04 ... -3.47328978e-03\n",
            "   -4.19691624e-03 -1.60262408e-03]\n",
            "  ...\n",
            "  [-5.90822054e-03 -9.01209284e-03  2.03801040e-03 ...  7.10983342e-03\n",
            "   -1.06048128e-02  5.11775678e-03]\n",
            "  [-6.65630028e-03 -1.51148047e-02  6.82310108e-03 ...  1.04148444e-02\n",
            "   -1.24072470e-02  5.23724826e-03]\n",
            "  [-4.06296365e-03 -9.14469268e-03  6.21652184e-03 ...  7.66411051e-03\n",
            "   -8.70406441e-03  2.50696205e-04]]\n",
            "\n",
            " [[ 3.91729176e-03  4.39045765e-03  1.48466916e-03 ... -2.28003133e-03\n",
            "    2.83433520e-03  3.97115014e-03]\n",
            "  [ 1.12011249e-03 -6.65033655e-03  6.47338759e-03 ...  2.78654997e-03\n",
            "   -2.09093071e-03  4.05469351e-03]\n",
            "  [-2.43487279e-03 -1.57546694e-03  3.32596269e-03 ...  3.56590841e-03\n",
            "   -4.42570279e-04  4.03321534e-03]\n",
            "  ...\n",
            "  [-5.54603804e-03 -1.11188227e-02  4.32030018e-03 ... -3.11715831e-03\n",
            "   -5.09137707e-03 -3.36950528e-03]\n",
            "  [-2.27393140e-03 -1.10181803e-02  8.52712989e-03 ...  1.25584437e-03\n",
            "   -8.69634748e-03 -3.92675772e-03]\n",
            "  [-7.72574567e-04 -1.26371533e-02  7.70427007e-03 ... -2.33104499e-03\n",
            "   -3.88756022e-03 -7.61255738e-04]]], shape=(64, 100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v05Ac-liBHFa",
        "outputId": "7ffd352c-8d31-43b4-9a25-348fc1bfb9fb"
      },
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-1.89703819e-03 -5.89703955e-03 -2.00884975e-03 ... -7.52128690e-05\n",
            "  -4.80467686e-03 -5.56411571e-04]\n",
            " [-3.99279204e-04 -6.33597793e-03  2.12126481e-03 ... -5.80863329e-04\n",
            "   6.82282320e-04 -1.62663600e-05]\n",
            " [ 1.48616137e-03 -1.05754624e-03  8.67999811e-03 ...  6.24921056e-04\n",
            "   7.11157976e-04  1.37365132e-03]\n",
            " ...\n",
            " [-2.03353423e-03 -8.39139987e-03  1.38957892e-02 ...  8.81619286e-03\n",
            "  -1.42411806e-03  1.87050540e-03]\n",
            " [-1.15967426e-03 -2.24682782e-03  1.02629755e-02 ...  1.29282465e-02\n",
            "  -2.53062695e-04  2.40949541e-03]\n",
            " [-1.05780887e-03 -7.89244287e-03  5.86100295e-03 ...  9.71492846e-03\n",
            "  -5.48229972e-03  2.95950507e-04]], shape=(100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ici9NoOPBJTP",
        "outputId": "28cf6737-7b9e-4fb7-f6ae-b42e5d3f8b82"
      },
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-1.8970382e-03 -5.8970395e-03 -2.0088498e-03 -1.7009813e-03\n",
            " -1.7281173e-03 -9.7410410e-04  3.6023695e-05 -1.9584175e-03\n",
            "  5.7569034e-03 -7.3547419e-03 -3.6751518e-03  5.0564448e-04\n",
            " -7.3721055e-03  4.3338881e-04  1.7578256e-03 -1.0598394e-03\n",
            " -1.5078395e-03  2.5690396e-03 -1.4045745e-03  3.1659852e-03\n",
            "  2.4116973e-03  6.0772789e-03 -2.0551322e-04 -4.1633910e-03\n",
            "  7.7431863e-03 -2.0731594e-03 -4.3868129e-03 -6.8986439e-03\n",
            "  2.8044730e-03  3.7375914e-03  1.2820869e-03  1.4251597e-03\n",
            " -1.2096168e-03 -7.5621400e-03 -1.6598463e-03 -1.0664815e-03\n",
            " -5.4423525e-03  5.2041872e-03  3.3196725e-03 -2.6109051e-03\n",
            " -1.0882326e-03 -8.9697284e-04 -3.9378689e-03 -1.5184967e-03\n",
            " -1.6085991e-03 -3.7351944e-03  5.1448029e-03 -1.4042285e-03\n",
            " -1.0214159e-03  2.9856840e-03  3.9570522e-03  4.9887509e-03\n",
            "  3.3206402e-03  2.7489793e-04  3.2778794e-03  6.9128373e-04\n",
            "  2.5028687e-03  1.5037522e-03 -3.9171795e-03  5.5924514e-03\n",
            " -4.2084986e-03 -2.1425807e-03 -7.5212869e-05 -4.8046769e-03\n",
            " -5.5641157e-04], shape=(65,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pmdf7tpvBL49",
        "outputId": "f4b6893c-f5ac-4496-d8e2-112879c43dd8"
      },
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "';Zlmd:nFi--Vt$kPReFXxcP-S-UmA-dMy3Rdq :QM\\n$tyWOfoMPWLvqml!ofQH:wAMuqkvPDCzD,NWjEqG:HCY!hqTRQwJ$dOBFX'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjeLszm4BTrs"
      },
      "source": [
        "we need to create a loss function that can compare that output to the expected output and give us some numeric value representing how close the two were."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx72pI78BTEb"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uks2oc2KBhmd"
      },
      "source": [
        "## **Compiling the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNYv9LyEBc77"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmMVHl3SBpWI"
      },
      "source": [
        "# Creating Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT3EvT_xBnMa"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ8yhiJmCG10"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3KKO1CSCGfW",
        "outputId": "aaf388cd-abb7-4e96-f9ad-99ceb57c4f58"
      },
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 33s 171ms/step - loss: 3.0649\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 31s 172ms/step - loss: 2.0625\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 31s 172ms/step - loss: 1.7635\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 1.5886\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 1.4816\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 1.4097\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 1.3577\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 1.3151\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 1.2766\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 1.2359\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 1.1995\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 1.1650\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 1.1256\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 1.0912\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 1.0489\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 1.0079\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 31s 172ms/step - loss: 0.9647\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 31s 172ms/step - loss: 0.9235\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.8807\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 0.8413\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 0.8030\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 0.7622\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 0.7281\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.6969\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.6657\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.6431\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 0.6154\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 0.5925\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.5767\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 0.5571\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.5431\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.5285\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.5130\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.5026\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 31s 172ms/step - loss: 0.4914\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.4816\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 31s 172ms/step - loss: 0.4745\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.4672\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.4582\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 31s 172ms/step - loss: 0.4530\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 31s 172ms/step - loss: 0.4442\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 0.4397\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 0.4325\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 0.4322\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 0.4290\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 31s 170ms/step - loss: 0.4246\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.4185\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.4155\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.4127\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 31s 171ms/step - loss: 0.4113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdH8EhGFCbXr"
      },
      "source": [
        "## Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXDbXXyXCd1x"
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boEJvy_vjLJQ"
      },
      "source": [
        "Once the model is finished training, we can find the **lastest checkpoint** that stores the models weights using the following line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsquSpNJCgyP"
      },
      "source": [
        "#find the lastest checkpoint that stores the models weights \n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmPPtbaTKF8d"
      },
      "source": [
        "We can load **any checkpoint** we want by specifying the exact file to load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "6AtKu-jbComh",
        "outputId": "1a1dd518-3ec3-42b8-9998-a679cfe6df0d"
      },
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-97614ead033d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./training_checkpoints/ckpt_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m       \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_is_hdf5_filepath\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2814\u001b[0;31m   return (filepath.endswith('.h5') or filepath.endswith('.keras') or\n\u001b[0m\u001b[1;32m   2815\u001b[0m           filepath.endswith('.hdf5'))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python._pywrap_checkpoint_reader.Checkp' object has no attribute 'endswith'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtj2yyk0C3Ww"
      },
      "source": [
        "# **Generating Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkEuWzTTC1wB"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmC4RFx0DD2r",
        "outputId": "bb4c9d7d-db74-487b-9395-b98ee2265051"
      },
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: romeo\n",
            "romeour mind.\n",
            "\n",
            "HERMIONE:\n",
            "No, by your leave:\n",
            "I know you to a merry earth.\n",
            "\n",
            "LORD BERKELEY:\n",
            "My Lord, I have consider'd in my mind\n",
            "With all prerogative: he is not there\n",
            "Rut unknow the lords of England's raison.\n",
            "\n",
            "ROMEO:\n",
            "A gentleman of mild home.\n",
            "\n",
            "GRUMIO:\n",
            "What should it have to say?\n",
            "\n",
            "LUCIO:\n",
            "My lord, here comes my master, so blow\n",
            "NGARET:\n",
            "O, that they stay, then happiness is several tender champion\n",
            "The purse thereof:\n",
            "Ell if they bear me hence in Plantagenet, which haste, man. Friar Lodowick speak it,\n",
            "As I intended of the lady-shining trembles:\n",
            "broody children at our blood\n",
            "Doth make me stay trutthen better than the melting news.\n",
            "\n",
            "GLOUCESTER:\n",
            "That you shall crim by you, it legs afroud\n",
            "And nothing carent like to night.\n",
            "I will my letter know your lordship came.\n",
            "\n",
            "ISABELLA:\n",
            "Anot a puissant man, I met the\n",
            "dow\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}